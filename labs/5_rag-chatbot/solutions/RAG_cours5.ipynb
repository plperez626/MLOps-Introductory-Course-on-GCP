{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Create RAG on GCP"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Objectifs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Au cours de ce TP, vous apprendrez à créer un RAG. \r\n",
    "\r\n",
    "Il s'agit de combiner récupération d'information et génération de texte pour produire des réponses précises en s'appuyant sur des sources externes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "On utilisera :\r\n",
    "- Vertex AI preview"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import librairies"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from vertexai.preview import rag\r\n",
    "from vertexai.preview.generative_models import GenerativeModel, Tool\r\n",
    "import vertexai\r\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\r\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variables"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "PROJECT_ID = \"A REMPLIR\" # Trouvé dans la selection de projet\r\n",
    "display_name = \"A REMPLIR\" # Nom à choisir\r\n",
    "paths = [\"A REMPLIR\"] # Path vers le bucket"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Initialize Vertex AI API once per session\r\n",
    "# Vertex AI RAG Engine ne fonctionne que us-central1 ou europe-west3\r\n",
    "vertexai.init(project=PROJECT_ID, location=\"europe-west3\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Création corpus du RAG\r\n",
    "\r\n",
    "Avec un model d'embedding"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Configure embedding model, for example \"text-embedding-004\".\r\n",
    "embedding_model_config = rag.EmbeddingModelConfig(\r\n",
    "    publisher_model=\"publishers/google/models/text-embedding-004\"\r\n",
    ")\r\n",
    "\r\n",
    "rag_corpus = rag.create_corpus(\r\n",
    "    display_name=display_name,\r\n",
    "    embedding_model_config=embedding_model_config,\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ajouter les fichiers au corpus"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "response = rag.import_files(\r\n",
    "    rag_corpus.name,\r\n",
    "    paths,\r\n",
    "    chunk_size=\"A REMPLIR\",  # Optional\r\n",
    "    chunk_overlap=\"A REMPLIR\",  # Optional\r\n",
    "    max_embedding_requests_per_min=\"A REMPLIR\",  # Optional\r\n",
    ")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test Retriever"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "response = rag.retrieval_query(\r\n",
    "    rag_resources=[\r\n",
    "        rag.RagResource(\r\n",
    "            rag_corpus=rag_corpus.name,\r\n",
    "        )\r\n",
    "    ],\r\n",
    "    text=\"What IA models are the most efficient ?\",\r\n",
    "    similarity_top_k=\"A REMPLIR\",  # Optional\r\n",
    "    vector_distance_threshold=\"A REMPLIR\",  # Optional\r\n",
    ")\r\n",
    "print(response)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Création d'un outil RAG"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Enhance generation\r\n",
    "# Create a RAG retrieval tool\r\n",
    "rag_retrieval_tool = Tool.from_retrieval(\r\n",
    "    retrieval=rag.Retrieval(\r\n",
    "        source=rag.VertexRagStore(\r\n",
    "            rag_resources=[\r\n",
    "                rag.RagResource(\r\n",
    "                    rag_corpus=rag_corpus.name,  # Currently only 1 corpus is allowed.\r\n",
    "                )\r\n",
    "            ],\r\n",
    "            similarity_top_k=\"A REMPLIR\",  # Optional\r\n",
    "            vector_distance_threshold=\"A REMPLIR\",  # Optional\r\n",
    "        ),\r\n",
    "    )\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create a gemini-pro model instance\r\n",
    "rag_model = GenerativeModel(\r\n",
    "    model_name=\"A REMPLIR\", tools=[rag_retrieval_tool]\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Test de la génération"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Generate response\r\n",
    "response = rag_model.generate_content(\"QUESTION ?\")\r\n",
    "print(response.text)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## PARTIE EVALUATION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Installation Libraries :\r\n",
    "\r\n",
    "Ragas => evaluation de RAG\r\n",
    "\r\n",
    "langchain_google_vertexai => Utiliser un modèle dans vertexAI compatible avec Ragas"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!pip install ragas langchain_google_vertexai"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation du retriever"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import ragas\r\n",
    "from ragas.metrics import (\r\n",
    "    answer_relevancy,\r\n",
    "    faithfulness,\r\n",
    "    context_recall,\r\n",
    "    context_precision,\r\n",
    "    answer_similarity,\r\n",
    "    answer_correctness\r\n",
    ")\r\n",
    "\r\n",
    "from ragas.evaluation import evaluate\r\n",
    "from datasets import Dataset\r\n",
    "from langchain_google_vertexai import ChatVertexAI, VertexAIEmbeddings"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "metrics_eval = [answer_relevancy, # Indique si la réponse générée est pertinente par rapport à la question posée\r\n",
    "        faithfulness,             # Vérifie si la réponse est basée uniquement sur les informations fournies dans les contextes récupérés\r\n",
    "        context_recall,           # Mesure si le système a récupéré tous les contextes pertinents pour la question\r\n",
    "        context_precision,        # Mesure si les contextes récupérés sont pertinents pour la question\r\n",
    "        answer_correctness]       # Compare la réponse générée à une réponse de référence pour évaluer sa justesse"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ragas a besoin d'un dataset avec quatre champs : \"question\", \"ground_truth\", \"answer\", \"contexts\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dict_dataset_eval = {\r\n",
    "    \"question\" : [\"What are the models launched by Deepseek ?\", \"How many Core Contributors ?\", \"Concerning only MMLU (Pass@1), what are the performance of OpenAI-o1-1217?\",],\r\n",
    "    \"ground_truth\" : [\"The models lauched by Deekseek are : Deepseek-r1-Zero, Deepseek-r1\", \"There are 18 Core Contributors\", \"The performance of OpenAI-o1-1217 on MMLU (Pass@1) is 91.8.\"],\r\n",
    "    \"answer\" : [],\r\n",
    "    \"contexts\" : []\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Fonctions qui remplit le dict_dataset_eval\r\n",
    "\r\n",
    "Récupère la question du dictionnaire, la pose au modèle avec l'outil RAG et stocke les réponses et le contexte dans les listes correspondantes"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def filling_datatset(dict_dataset_eval):\r\n",
    "    \r\n",
    "    for question in dict_dataset_eval[\"question\"]:\r\n",
    "        \r\n",
    "        response_retriever = rag.retrieval_query(\r\n",
    "            rag_resources=[\r\n",
    "                rag.RagResource(\r\n",
    "                    rag_corpus=rag_corpus.name,\r\n",
    "                )\r\n",
    "            ],\r\n",
    "            text=question,\r\n",
    "            similarity_top_k=5,  # Optional\r\n",
    "            vector_distance_threshold=0.5,  # Optional\r\n",
    "        )\r\n",
    "        \r\n",
    "        retrieved_contexts = [context.text for context in response_retriever.contexts.contexts]\r\n",
    "        \r\n",
    "        dict_dataset_eval[\"contexts\"].append(retrieved_contexts)\r\n",
    "        \r\n",
    "        response_generator = rag_model.generate_content(question)\r\n",
    "        \r\n",
    "        dict_dataset_eval[\"answer\"].append(response_generator.text)        \r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "filling_datatset(dict_dataset_eval)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ragas a besoin d'un dataset :"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dataset_eval = Dataset.from_dict(dict_dataset_eval)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Création d'un modèle génératif et d'un modèle d'embeddings"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vertextai_llm = ChatVertexAI(\r\n",
    "    model_name=\"gemini-1.5-flash-001\"\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "vertextai_embeddings = VertexAIEmbeddings(\r\n",
    "    model_name=\"textembedding-gecko\"\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation : \r\n",
    "\r\n",
    "PS : La fonction evaluate() compare les réponses générées avec le RAG et le ground_truth associé avec le modèle vertextai_llm et compare la question et le contexte retrouvé avec vertextai_embeddings.\r\n",
    "\r\n",
    "Le modèle va recevoir beaucoup d'appels par minutes ce qui peut provoquer le message d'erreur : \"Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\" \r\n",
    "\r\n",
    "Il faut laisser tourner le run, les appels font se faire."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ragas_eval_with_gt = evaluate(dataset_eval, metrics_eval, llm=vertextai_llm, embeddings=vertextai_embeddings)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Résultats :"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ragas_eval_with_gt"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.10.11 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "interpreter": {
   "hash": "99432052f818d30113247392634a2d7602c95033d0482a5a7cbfa95c3f23ec21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}