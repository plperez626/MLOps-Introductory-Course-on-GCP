{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## RAG"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "from vertexai.preview import rag\r\n",
    "from vertexai.preview.generative_models import GenerativeModel, Tool\r\n",
    "import vertexai\r\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\r\n",
    "import time"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "PROJECT_ID = \"projet-ia-448520\" # Trouv√© dans la selection de projet\r\n",
    "display_name = \"test_corpus\"\r\n",
    "paths = [\"gs://cours5bucket\"] #Path vers le bucket\r\n",
    "# paths = [\"https://drive.google.com/file/d/123\", \"gs://my_bucket/my_files_dir\"]  # Supports Google Cloud Storage and Google Drive Links"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "# Initialize Vertex AI API once per session\r\n",
    "# Vertex AI RAG Engine ne fonctionne que us-central1 ou europe-west3\r\n",
    "vertexai.init(project=PROJECT_ID, location=\"europe-west3\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# Create RagCorpus\r\n",
    "# Configure embedding model, for example \"text-embedding-004\".\r\n",
    "embedding_model_config = rag.EmbeddingModelConfig(\r\n",
    "    publisher_model=\"publishers/google/models/text-embedding-004\"\r\n",
    ")\r\n",
    "\r\n",
    "rag_corpus = rag.create_corpus(\r\n",
    "    display_name=display_name,\r\n",
    "    embedding_model_config=embedding_model_config,\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# Import Files to the RagCorpus\r\n",
    "response = rag.import_files(\r\n",
    "    rag_corpus.name,\r\n",
    "    paths,\r\n",
    "    chunk_size=512,  # Optional\r\n",
    "    chunk_overlap=100,  # Optional\r\n",
    "    max_embedding_requests_per_min=900,  # Optional\r\n",
    ")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# Direct context retrieval\r\n",
    "response = rag.retrieval_query(\r\n",
    "    rag_resources=[\r\n",
    "        rag.RagResource(\r\n",
    "            rag_corpus=rag_corpus.name,\r\n",
    "            # Optional: supply IDs from `rag.list_files()`.\r\n",
    "            # rag_file_ids=[\"rag-file-1\", \"rag-file-2\", ...],\r\n",
    "        )\r\n",
    "    ],\r\n",
    "    text=\"What IA models are the most efficient ?\",\r\n",
    "    similarity_top_k=5,  # Optional\r\n",
    "    vector_distance_threshold=0.5,  # Optional\r\n",
    ")\r\n",
    "print(response)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "contexts {\n",
      "  contexts {\n",
      "    source_uri: \"gs://cours5bucket/deepseek-r1 paper.txt\"\n",
      "    text: \"Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor-\\nmance based on official reports. For distilled models, we also compare the open-source model\\nQwQ-32B-Preview (Qwen, 2024a).\\nEvaluation Setup We set the maximum generation length to 32,768 tokens for the models.\\nWe found that using greedy decoding to evaluate long-output reasoning models results in\\nhigher repetition rates and significant variability across different checkpoints. Therefore, we\\ndefault to pass@ \\360\\235\\221\\230evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature.\\nSpecifically, we use a sampling temperature of 0.6and a top- \\360\\235\\221\\235value of 0.95 to generate \\360\\235\\221\\230\\nresponses (typically between 4and 64, depending on the test set size) for each question. Pass@1\\nis then calculated as\\npass@1 =1\\n\\360\\235\\221\\230\\360\\235\\221\\230\\342\\210\\221\\357\\270\\201\\n\\360\\235\\221\\226=1\\360\\235\\221\\235\\360\\235\\221\\226,\\nwhere\\360\\235\\221\\235\\360\\235\\221\\226denotes the correctness of the \\360\\235\\221\\226-th response. This method provides more reliable\\nperformance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang\\net al., 2022) using 64 samples, denoted as cons@64.\\n1https://aider.chat\\n2https://codeforces.com\\n3https://www.cms.org.cn/Home/comp/comp/cid/12.html\\n123.1.\"\n",
      "    distance: 0.478317147686249\n",
      "    source_display_name: \"deepseek-r1 paper.txt\"\n",
      "    score: 0.478317147686249\n",
      "  }\n",
      "  contexts {\n",
      "    source_uri: \"gs://cours5bucket/deepseek-r1 paper.pdf\"\n",
      "    text: \"Since accessing the OpenAI-o1-1217 API is challenging in mainland China, we report its perfor\\357\\277\\275mance based on official reports. For distilled models, we also compare the open-source model\\r\\nQwQ-32B-Preview (Qwen, 2024a).\\r\\nEvaluation Setup We set the maximum generation length to 32,768 tokens for the models.\\r\\nWe found that using greedy decoding to evaluate long-output reasoning models results in\\r\\nhigher repetition rates and significant variability across different checkpoints. Therefore, we\\r\\ndefault to pass@\\360\\235\\221\\230 evaluation (Chen et al., 2021) and report pass@1 using a non-zero temperature.\\r\\nSpecifically, we use a sampling temperature of 0.6 and a top-\\360\\235\\221\\235 value of 0.95 to generate \\360\\235\\221\\230\\r\\nresponses (typically between 4 and 64, depending on the test set size) for each question. Pass@1\\r\\nis then calculated as\\r\\npass@1 =\\r\\n1\\r\\n\\360\\235\\221\\230\\r\\n\\342\\210\\221\\357\\270\\201\\r\\n\\360\\235\\221\\230\\r\\n\\360\\235\\221\\226=1\\r\\n\\360\\235\\221\\235\\360\\235\\221\\226\\r\\n,\\r\\nwhere \\360\\235\\221\\235\\360\\235\\221\\226 denotes the correctness of the \\360\\235\\221\\226-th response. This method provides more reliable\\r\\nperformance estimates. For AIME 2024, we also report consensus (majority vote) results (Wang\\r\\net al., 2022) using 64 samples, denoted as cons@64.\\r\\n1https://aider.chat\\r\\n2https://codeforces.com\\r\\n3https://www.cms.org.cn/Home/comp/comp/cid/12.html\\r\\n123.1.\"\n",
      "    distance: 0.47843456191858724\n",
      "    source_display_name: \"deepseek-r1 paper.pdf\"\n",
      "    score: 0.47843456191858724\n",
      "  }\n",
      "  contexts {\n",
      "    source_uri: \"gs://cours5bucket/deepseek-r1 paper.txt\"\n",
      "    text: \"RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-\\nDistill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than\\nDeepSeek-R1-Zero-Qwen-32B across all benchmarks.\\nTherefore, we can draw two conclusions: First, distilling more powerful models into smaller\\nones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in\\nthis paper require enormous computational power and may not even achieve the performance\\nof distillation. Second, while distillation strategies are both economical and effective, advancing\\nbeyond the boundaries of intelligence may still require more powerful base models and larger-\\nscale reinforcement learning.\\n4.2. Unsuccessful Attempts\\nIn the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along\\nthe way. We share our failure experiences here to provide insights, but this does not imply that\\nthese approaches are incapable of developing effective reasoning models.\\nProcess Reward Model (PRM) PRM is a reasonable method to guide the model toward better\\napproaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al.,\\n2023). However, in practice, PRM has three main limitations that may hinder its ultimate suc-\\ncess. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second,\\ndetermining whether the current intermediate step is correct is a challenging task. Automated\\nannotation using models may not yield satisfactory results, while manual annotation is not con-\\nducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward\\nhacking (Gao et al., 2022), and retraining the reward model needs additional training resources\\nand it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good\\nability to rerank the top-N responses generated by the model or assist in guided search (Snell\\net al., 2024), its advantages are limited compared to the additional computational overhead it\\nintroduces during the large-scale reinforcement learning process in our experiments.\"\n",
      "    distance: 0.48443362212746777\n",
      "    source_display_name: \"deepseek-r1 paper.txt\"\n",
      "    score: 0.48443362212746777\n",
      "  }\n",
      "  contexts {\n",
      "    source_uri: \"gs://cours5bucket/deepseek-r1 paper.pdf\"\n",
      "    text: \"RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-\\r\\nDistill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than\\r\\nDeepSeek-R1-Zero-Qwen-32B across all benchmarks.\\r\\nTherefore, we can draw two conclusions: First, distilling more powerful models into smaller\\r\\nones yields excellent results, whereas smaller models relying on the large-scale RL mentioned in\\r\\nthis paper require enormous computational power and may not even achieve the performance\\r\\nof distillation. Second, while distillation strategies are both economical and effective, advancing\\r\\nbeyond the boundaries of intelligence may still require more powerful base models and larger\\357\\277\\275scale reinforcement learning.\\r\\n4.2. Unsuccessful Attempts\\r\\nIn the early stages of developing DeepSeek-R1, we also encountered failures and setbacks along\\r\\nthe way. We share our failure experiences here to provide insights, but this does not imply that\\r\\nthese approaches are incapable of developing effective reasoning models.\\r\\nProcess Reward Model (PRM) PRM is a reasonable method to guide the model toward better\\r\\napproaches for solving reasoning tasks (Lightman et al., 2023; Uesato et al., 2022; Wang et al.,\\r\\n2023). However, in practice, PRM has three main limitations that may hinder its ultimate suc\\357\\277\\275cess. First, it is challenging to explicitly define a fine-grain step in general reasoning. Second,\\r\\ndetermining whether the current intermediate step is correct is a challenging task. Automated\\r\\nannotation using models may not yield satisfactory results, while manual annotation is not con\\357\\277\\275ducive to scaling up. Third, once a model-based PRM is introduced, it inevitably leads to reward\\r\\nhacking (Gao et al., 2022), and retraining the reward model needs additional training resources\\r\\nand it complicates the whole training pipeline. In conclusion, while PRM demonstrates a good\\r\\nability to rerank the top-N responses generated by the model or assist in guided search (Snell\\r\\net al., 2024), its advantages are limited compared to the additional computational overhead it\\r\\nintroduces during the large-scale reinforcement learning process in our experiments.\"\n",
      "    distance: 0.4848456955830336\n",
      "    source_display_name: \"deepseek-r1 paper.pdf\"\n",
      "    score: 0.4848456955830336\n",
      "  }\n",
      "  contexts {\n",
      "    source_uri: \"gs://cours5bucket/deepseek-r1 paper.pdf\"\n",
      "    text: \"In conclusion, while PRM demonstrates a good\\r\\nability to rerank the top-N responses generated by the model or assist in guided search (Snell\\r\\net al., 2024), its advantages are limited compared to the additional computational overhead it\\r\\nintroduces during the large-scale reinforcement learning process in our experiments.\\r\\nMonte Carlo Tree Search (MCTS) Inspired by AlphaGo (Silver et al., 2017b) and AlphaZero (Sil\\357\\277\\275ver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time\\r\\ncompute scalability. This approach involves breaking answers into smaller parts to allow the\\r\\nmodel to explore the solution space systematically. To facilitate this, we prompt the model to\\r\\ngenerate multiple tags that correspond to specific reasoning steps necessary for the search. For\\r\\ntraining, we first use collected prompts to find answers via MCTS guided by a pre-trained value\\r\\nmodel. Subsequently, we use the resulting question-answer pairs to train both the actor model\\r\\nand the value model, iteratively refining the process.\\r\\nHowever, this approach encounters several challenges when scaling up the training. First,\\r\\nunlike chess, where the search space is relatively well-defined, token generation presents an\\r\\n15exponentially larger search space. To address this, we set a maximum extension limit for each\\r\\nnode, but this can lead to the model getting stuck in local optima. Second, the value model\\r\\ndirectly influences the quality of generation since it guides each step of the search process.\\r\\nTraining a fine-grained value model is inherently difficult, which makes it challenging for the\\r\\nmodel to iteratively improve. While AlphaGo\\342\\200\\231s core success relied on training a value model to\\r\\nprogressively enhance its performance, this principle proves difficult to replicate in our setup\\r\\ndue to the complexities of token generation.\\r\\nIn conclusion, while MCTS can improve performance during inference when paired with a\\r\\npre-trained value model, iteratively boosting model performance through self-search remains a\\r\\nsignificant challenge.\\r\\n5. Conclusion, Limitations, and Future Work\\r\\nIn this work, we share our journey in enhancing model reasoning abilities through reinforcement\\r\\nlearning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start\\r\\ndata, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,\\r\\nleveraging cold-start data alongside iterative RL fine-tuning.\"\n",
      "    distance: 0.4952581179475072\n",
      "    source_display_name: \"deepseek-r1 paper.pdf\"\n",
      "    score: 0.4952581179475072\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "# Enhance generation\r\n",
    "# Create a RAG retrieval tool\r\n",
    "rag_retrieval_tool = Tool.from_retrieval(\r\n",
    "    retrieval=rag.Retrieval(\r\n",
    "        source=rag.VertexRagStore(\r\n",
    "            rag_resources=[\r\n",
    "                rag.RagResource(\r\n",
    "                    rag_corpus=rag_corpus.name,  # Currently only 1 corpus is allowed.\r\n",
    "                    # Optional: supply IDs from `rag.list_files()`.\r\n",
    "                    # rag_file_ids=[\"rag-file-1\", \"rag-file-2\", ...],\r\n",
    "                )\r\n",
    "            ],\r\n",
    "            similarity_top_k=3,  # Optional\r\n",
    "            vector_distance_threshold=0.5,  # Optional\r\n",
    "        ),\r\n",
    "    )\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "# Create a gemini-pro model instance\r\n",
    "rag_model = GenerativeModel(\r\n",
    "    model_name=\"gemini-1.5-flash-001\", tools=[rag_retrieval_tool]\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "# Generate response\r\n",
    "response = rag_model.generate_content(\"What IA models are the most efficient ?\")\r\n",
    "print(response.text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The paper \"DeepSeek-R1\" highlights the efficiency of distilled models, particularly DeepSeek-R1-7B. This model outperforms non-reasoning models like GPT-4o-0513. Further distillation of larger models, such as DeepSeek-R1-14B, DeepSeek-R1-32B, and DeepSeek-R1-70B, leads to significant improvements on various benchmarks. The study emphasizes that while reinforcement learning can be applied to distilled models to improve their performance, even simple SFT-distilled models show promising results. \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EVALUATION"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation retriever"
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "!pip install ragas langchain_google_vertexai"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Requirement already satisfied: ragas in /opt/conda/lib/python3.10/site-packages (0.2.13)\n",
      "Requirement already satisfied: langchain_google_vertexai in /opt/conda/lib/python3.10/site-packages (2.0.13)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from ragas) (1.26.4)\n",
      "Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (from ragas) (3.3.0)\n",
      "Requirement already satisfied: tiktoken in /opt/conda/lib/python3.10/site-packages (from ragas) (0.9.0)\n",
      "Requirement already satisfied: langchain in /opt/conda/lib/python3.10/site-packages (from ragas) (0.3.18)\n",
      "Requirement already satisfied: langchain-core in /opt/conda/lib/python3.10/site-packages (from ragas) (0.3.35)\n",
      "Requirement already satisfied: langchain-community in /opt/conda/lib/python3.10/site-packages (from ragas) (0.3.17)\n",
      "Requirement already satisfied: langchain_openai in /opt/conda/lib/python3.10/site-packages (from ragas) (0.3.6)\n",
      "Requirement already satisfied: nest-asyncio in /opt/conda/lib/python3.10/site-packages (from ragas) (1.6.0)\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.10/site-packages (from ragas) (1.4.4)\n",
      "Requirement already satisfied: pydantic>=2 in /opt/conda/lib/python3.10/site-packages (from ragas) (2.10.6)\n",
      "Requirement already satisfied: openai>1 in /opt/conda/lib/python3.10/site-packages (from ragas) (1.63.1)\n",
      "Requirement already satisfied: diskcache>=5.6.3 in /opt/conda/lib/python3.10/site-packages (from ragas) (5.6.3)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.76.0 in /opt/conda/lib/python3.10/site-packages (from langchain_google_vertexai) (1.80.0)\n",
      "Requirement already satisfied: google-cloud-storage<3.0.0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from langchain_google_vertexai) (2.19.0)\n",
      "Requirement already satisfied: httpx<0.29.0,>=0.28.0 in /opt/conda/lib/python3.10/site-packages (from langchain_google_vertexai) (0.28.1)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from langchain_google_vertexai) (0.4.0)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (2.24.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (2.37.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (1.25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (3.20.3)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (24.2)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (3.25.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (1.14.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (2.0.6)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (4.12.2)\n",
      "Requirement already satisfied: docstring-parser<1 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (0.16)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai) (2.7.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai) (2.32.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai) (1.6.0)\n",
      "Requirement already satisfied: anyio in /opt/conda/lib/python3.10/site-packages (from httpx<0.29.0,>=0.28.0->langchain_google_vertexai) (4.7.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from httpx<0.29.0,>=0.28.0->langchain_google_vertexai) (2024.12.14)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.10/site-packages (from httpx<0.29.0,>=0.28.0->langchain_google_vertexai) (1.0.7)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from httpx<0.29.0,>=0.28.0->langchain_google_vertexai) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/conda/lib/python3.10/site-packages (from httpcore==1.*->httpx<0.29.0,>=0.28.0->langchain_google_vertexai) (0.14.0)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /opt/conda/lib/python3.10/site-packages (from langchain-core->ragas) (0.3.8)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /opt/conda/lib/python3.10/site-packages (from langchain-core->ragas) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/conda/lib/python3.10/site-packages (from langchain-core->ragas) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /opt/conda/lib/python3.10/site-packages (from langchain-core->ragas) (6.0.2)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/conda/lib/python3.10/site-packages (from openai>1->ragas) (1.9.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from openai>1->ragas) (0.8.2)\n",
      "Requirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from openai>1->ragas) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /opt/conda/lib/python3.10/site-packages (from openai>1->ragas) (4.67.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2->ragas) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/conda/lib/python3.10/site-packages (from pydantic>=2->ragas) (2.27.2)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (15.0.2)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->ragas) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (3.11.11)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /opt/conda/lib/python3.10/site-packages (from datasets->ragas) (0.28.1)\n",
      "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.6 in /opt/conda/lib/python3.10/site-packages (from langchain->ragas) (0.3.6)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/conda/lib/python3.10/site-packages (from langchain->ragas) (2.0.36)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/conda/lib/python3.10/site-packages (from langchain->ragas) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/conda/lib/python3.10/site-packages (from langchain-community->ragas) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /opt/conda/lib/python3.10/site-packages (from langchain-community->ragas) (2.7.1)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/conda/lib/python3.10/site-packages (from tiktoken->ragas) (2024.11.6)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->ragas) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->ragas) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->ragas) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->ragas) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->ragas) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->ragas) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets->ragas) (1.18.3)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/conda/lib/python3.10/site-packages (from anyio->httpx<0.29.0,>=0.28.0->langchain_google_vertexai) (1.2.2)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (1.66.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (1.68.1)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /opt/conda/lib/python3.10/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (1.48.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (4.9)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (2.9.0.post0)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.10/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (0.13.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/conda/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core->ragas) (3.10.15)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core->ragas) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from langsmith<0.4,>=0.1.125->langchain-core->ragas) (0.23.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas) (1.0.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai) (3.4.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage<3.0.0,>=2.18.0->langchain_google_vertexai) (1.26.20)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.1.1)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->ragas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets->ragas) (2024.2)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (0.6.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil<3.0dev,>=2.7.2->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.76.0->langchain_google_vertexai) (1.17.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "import ragas\r\n",
    "from ragas.metrics import (\r\n",
    "    answer_relevancy,\r\n",
    "    faithfulness,\r\n",
    "    context_recall,\r\n",
    "    context_precision,\r\n",
    "    answer_similarity,\r\n",
    "    answer_correctness\r\n",
    ")\r\n",
    "\r\n",
    "from ragas.evaluation import evaluate\r\n",
    "from datasets import Dataset\r\n",
    "from langchain_google_vertexai import ChatVertexAI, VertexAIEmbeddings\r\n",
    "# from ragas.llms import LangchainLLMWrapper"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "metrics_eval = [answer_relevancy, \r\n",
    "        faithfulness, \r\n",
    "        context_recall,       \r\n",
    "        context_precision,\r\n",
    "        answer_correctness]"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Ragas a besoin d'un dataset avec quatre champs : \"question\", \"ground_truth\", \"answer\", \"contexts\""
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "dict_dataset_eval = {\r\n",
    "    \"question\" : [\"What are the models launched by Deepseek ?\", \"How many Core Contributors ?\", \"Concerning only MMLU (Pass@1), what are the performance of OpenAI-o1-1217?\",],\r\n",
    "    \"ground_truth\" : [\"Deepseek-r1-Zero, Deepseek-r1\", \"18\", \"The performance of OpenAI-o1-1217 on MMLU (Pass@1) is 91.8.\"],\r\n",
    "    \"answer\" : [],\r\n",
    "    \"contexts\" : []\r\n",
    "}"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def filling_datatset(dict_dataset_eval):\r\n",
    "    \r\n",
    "    for question in dict_dataset_eval[\"question\"]:\r\n",
    "        \r\n",
    "        response_retriever = rag.retrieval_query(\r\n",
    "            rag_resources=[\r\n",
    "                rag.RagResource(\r\n",
    "                    rag_corpus=rag_corpus.name,\r\n",
    "                    # Optional: supply IDs from `rag.list_files()`.\r\n",
    "                    # rag_file_ids=[\"rag-file-1\", \"rag-file-2\", ...],\r\n",
    "                )\r\n",
    "            ],\r\n",
    "            text=question,\r\n",
    "            similarity_top_k=5,  # Optional\r\n",
    "            vector_distance_threshold=0.5,  # Optional\r\n",
    "        )\r\n",
    "        \r\n",
    "        retrieved_contexts = [context.text for context in response_retriever.contexts.contexts]\r\n",
    "        \r\n",
    "        dict_dataset_eval[\"contexts\"].append(retrieved_contexts)\r\n",
    "        \r\n",
    "        response_generator = rag_model.generate_content(question)\r\n",
    "        \r\n",
    "        dict_dataset_eval[\"answer\"].append(response_generator.text)        \r\n",
    "        "
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "filling_datatset(dict_dataset_eval)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "dataset_eval = Dataset.from_dict(dict_dataset_eval)"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "vertextai_llm = ChatVertexAI(\r\n",
    "    model_name=\"gemini-1.5-flash-001\"\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "vertextai_embeddings = VertexAIEmbeddings(\r\n",
    "    model_name=\"textembedding-gecko\"\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "ragas_eval_with_gt = evaluate(dataset_eval, metrics_eval, llm=vertextai_llm, embeddings=vertextai_embeddings)"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cce89763003a4a84ae010775fc7f059c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 8.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 10.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n",
      "Retrying langchain_google_vertexai.chat_models._acompletion_with_retry.<locals>._completion_with_retry_inner in 4.0 seconds as it raised ResourceExhausted: 429 Quota exceeded for aiplatform.googleapis.com/generate_content_requests_per_minute_per_project_per_base_model with base model: gemini-1.5-flash. Please submit a quota increase request. https://cloud.google.com/vertex-ai/docs/generative-ai/quotas-genai..\n"
     ]
    }
   ],
   "metadata": {
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "ragas_eval_with_gt"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'answer_relevancy': 0.9117, 'faithfulness': 1.0000, 'context_recall': 1.0000, 'context_precision': 0.6222, 'answer_correctness': 0.5135}"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "metadata": {
    "tags": []
   }
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}