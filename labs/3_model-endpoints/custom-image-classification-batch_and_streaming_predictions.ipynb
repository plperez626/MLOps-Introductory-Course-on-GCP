{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "title"
   },
   "source": [
    "# TP 3 : job d'entraînement, déploiement de modèle, prédiction par batch et en ligne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "objective:custom,training,online_prediction"
   },
   "source": [
    "### Objectifs\n",
    "\n",
    "Au cours de ce TP, vous apprendrez à créer un modèle de Vision personnalisé, vous configurerez et lancerez un job d'entraînement, enfin, vous déploierez le modèle pour de la prédiction par lot (batch) puis pour de la prédiction en ligne (streaming). \n",
    "\n",
    "On utilisera les services Google Cloud ML suivant :\n",
    "\n",
    "- Vertex AI Training\n",
    "- Vertex AI Batch Prediction\n",
    "- Vertex AI Model resource\n",
    "\n",
    "Quelques liens utiles :\n",
    "- [Custom training](https://cloud.google.com/vertex-ai/docs/training/custom-training) \n",
    "- [Vertex AI Batch Prediction](https://cloud.google.com/vertex-ai/docs/tabular-data/classification-regression/get-batch-predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dataset:custom,cifar10,icn"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "TensorFlow met à disposition de nombreux datasets issues de domaines variés et pensés pour le machine learning : [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/overview). \n",
    "\n",
    "Pour mettre tout ceci en pratique, on utilisera les données [cifar10 dataset](https://www.tensorflow.org/datasets/catalog/cifar10) et on développera un modèle de Computer Vision avec TensorFlow afin de prédire la nature des objets présents sur les images parmi 10 catégories possibles : avion, voiture, oiseau, chat, cerf, chien, grenouille, cheval, bateau, camion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3b1ffd5ab768"
   },
   "source": [
    "# Mise en place de l'environnement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aae9ca040eab"
   },
   "source": [
    "### Installer Vertex AI SDK for Python et autres\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "23e23ce735c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (1.79.0)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Using cached google_cloud_aiplatform-1.81.0-py2.py3-none-any.whl.metadata (32 kB)\n",
      "Requirement already satisfied: google-cloud-storage in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (2.19.0)\n",
      "Collecting google-cloud-storage\n",
      "  Using cached google_cloud_storage-3.0.0-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pillow in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (11.1.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.2.3-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (2.24.1)\n",
      "Requirement already satisfied: google-auth<3.0.0dev,>=2.14.1 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (2.38.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (1.26.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (4.25.3)\n",
      "Requirement already satisfied: packaging>=14.3 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (23.1)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (3.29.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (1.14.0)\n",
      "Requirement already satisfied: shapely<3.0.0dev in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (2.0.7)\n",
      "Requirement already satisfied: pydantic<3 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (1.10.12)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (4.12.2)\n",
      "Requirement already satisfied: docstring-parser<1 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-cloud-aiplatform) (0.16)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-cloud-storage) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.7.2 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-cloud-storage) (2.7.2)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-cloud-storage) (2.32.3)\n",
      "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-cloud-storage) (1.6.0)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.63.0)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.70.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.34.1->google-cloud-aiplatform) (1.62.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (4.9)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.3 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.14.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2024.2.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0dev,>=2.14.1->google-cloud-aiplatform) (0.4.8)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ahochedez\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil<3.0dev,>=2.7.3->google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (1.16.0)\n",
      "Using cached google_cloud_aiplatform-1.81.0-py2.py3-none-any.whl (7.3 MB)\n",
      "Using cached numpy-2.2.3-cp311-cp311-win_amd64.whl (12.9 MB)\n",
      "Installing collected packages: numpy, google-cloud-aiplatform\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.79.0\n",
      "    Uninstalling google-cloud-aiplatform-1.79.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.79.0\n",
      "Successfully installed google-cloud-aiplatform-1.81.0 numpy-2.2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ahochedez\\AppData\\Local\\Temp\\pip-uninstall-rr1z35t6'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\ahochedez\\AppData\\Local\\anaconda3\\Lib\\site-packages\\~-mpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.0 requires FuzzyTM>=0.4.0, which is not installed.\n",
      "astropy 5.3.4 requires numpy<2,>=1.21, but you have numpy 2.2.3 which is incompatible.\n",
      "contourpy 1.2.0 requires numpy<2.0,>=1.20, but you have numpy 2.2.3 which is incompatible.\n",
      "matplotlib 3.8.0 requires numpy<2,>=1.21, but you have numpy 2.2.3 which is incompatible.\n",
      "numba 0.59.0 requires numpy<1.27,>=1.22, but you have numpy 2.2.3 which is incompatible.\n",
      "pywavelets 1.5.0 requires numpy<2.0,>=1.22.4, but you have numpy 2.2.3 which is incompatible.\n",
      "scipy 1.11.4 requires numpy<1.28.0,>=1.21.6, but you have numpy 2.2.3 which is incompatible.\n",
      "streamlit 1.37.0 requires pillow<11,>=7.1.0, but you have pillow 11.1.0 which is incompatible.\n",
      "transformers4rec 23.12.0 requires transformers[torch]<4.31.0,>=4.12, but you have transformers 4.48.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "! pip3 install --upgrade google-cloud-aiplatform \\\n",
    "                        google-cloud-storage \\\n",
    "                        pillow  \\\n",
    "                        numpy   \\\n",
    "                        backports.tarfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "107c51893a64"
   },
   "source": [
    "### Variables du projet Google Cloud and initialisation du SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "294fe4e5a671"
   },
   "outputs": [],
   "source": [
    "PROJECT_ID = \"projet-ia-448520\"  # @param {type:\"string\"}\n",
    "LOCATION = \"us-central1\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ddbea904fbe5"
   },
   "source": [
    "#### Renseigner le chemin du Cloud Storage bucket\n",
    "\n",
    "Le bucket permettra de stocker les datasets et le modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "751138cf3bd5"
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://cours3bucket\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "58cb4f5895f0"
   },
   "source": [
    "**Si vous n'avez pas de bucket** : Décommentez la cellule suivante afin de créer un bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5e1288505682"
   },
   "outputs": [],
   "source": [
    "#! gsutil mb -l $LOCATION -p $PROJECT_ID $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fb03963cdb69"
   },
   "source": [
    "#### Initialisation du SDK Python\n",
    "\n",
    "Le SDK python permet d'utiliser l'API de Google Cloud Platform avec du code plutôt qu'avec l'interface utilisateur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "a4f61991b160"
   },
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project=PROJECT_ID, location=LOCATION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_vars"
   },
   "source": [
    "### Import des librairies python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "9dcd3eedadfb"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "container:training,prediction"
   },
   "source": [
    "### Mise en place des conteneurs docker pré-construit\n",
    "\n",
    "On utilisera ici des conteneurs dockeurs pré-construits, fournis par Vertex AI, qui permettent à nos codes d'entrainement et de prédiction de tourner sans encombre sur les machines virtuelles de GCP. \n",
    "\n",
    "Puisque l'on utilise des conteneurs pré-construits, il suffit de fournir un code python pour l'entrainement puis de préciser le nom de deux conteneurs parmi les listes suivantes : [Pre-built containers for training](https://cloud.google.com/vertex-ai/docs/training/pre-built-containers) et [Pre-built containers for prediction](https://cloud.google.com/vertex-ai/docs/predictions/pre-built-containers), en prenant garde à ce que ces conteneurs fournissent des environnements adaptés à notre projet et aux machines que l'on souhaite utiliser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "1u1mr18jlugv"
   },
   "outputs": [],
   "source": [
    "TRAIN_VERSION = \"tf-cpu.2-9\"\n",
    "DEPLOY_VERSION = \"tf2-cpu.2-9\"\n",
    "\n",
    "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/{}:latest\".format(TRAIN_VERSION)\n",
    "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(DEPLOY_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le code pour l'inférence n'aura pas à être écrit. En effet, le conteneur de prédiction pré-construit contient un serveur d'inférence qui chargera les poids de notre modèle entrainé pour servir des prédictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tutorial_start:custom"
   },
   "source": [
    "# Entraînement d'un modèle\n",
    "\n",
    "Nous allons maintenant créer un modèle de vision et nous l'entraînerons sur les données CIFAR10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "taskpy_contents"
   },
   "source": [
    "#### Écrire un script d'entrainement\n",
    "\n",
    "La prochaine cellule vise à écrire le fichier `task.py` qui contient le script de définition et d'entrainement de notre modèle. On suit les étapes suivante :\n",
    "\n",
    "- Récuperer les paramètres de l'entrainement en parsant les arguments. \n",
    "- Récupérer le chemin du répertoire `AIP_MODEL_DIR` où les artefacts de notre modèle seront enregistrés. La variable sera fixée par le service d'entrainement.\n",
    "- Charger le CIFAR10 dataset.\n",
    "- Définir un modèle CNN avec tensortlow.Keras.\n",
    "- Compiler le modèle (`compile()`).\n",
    "- Mettre en place la statégie de distribution des calculs précisée par `args.distribute`.\n",
    "- Entrainer le modèle (`fit()`) avec les paramètres : `args.epochs` et `args.steps`\n",
    "- Sauvegarder le modèle à l'emplacement spécifié (`save(MODEL_DIR)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "72rUqXNFlugx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task.py\n",
    "# Single, Mirror and Multi-Machine Distributed Training for CIFAR-10\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "tfds.disable_progress_bar()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lr', dest='lr',\n",
    "                    default=0.01, type=float,\n",
    "                    help='Learning rate.')\n",
    "parser.add_argument('--epochs', dest='epochs',\n",
    "                    default=10, type=int,\n",
    "                    help='Number of epochs.')\n",
    "parser.add_argument('--steps', dest='steps',\n",
    "                    default=200, type=int,\n",
    "                    help='Number of steps per epoch.')\n",
    "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
    "                    help='distributed training strategy')\n",
    "args = parser.parse_args()\n",
    "\n",
    "print('Python Version = {}'.format(sys.version))\n",
    "print('TensorFlow Version = {}'.format(tf.__version__))\n",
    "print('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "print('DEVICES', device_lib.list_local_devices())\n",
    "\n",
    "# Single Machine, single compute device\n",
    "if args.distribute == 'single':\n",
    "    if tf.test.is_gpu_available():\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "# Single Machine, multiple compute device\n",
    "elif args.distribute == 'mirror':\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "# Multiple Machine, multiple compute device\n",
    "elif args.distribute == 'multi':\n",
    "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "\n",
    "# Multi-worker configuration\n",
    "print('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Preparing dataset\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "def make_datasets_unbatched():\n",
    "  # Scaling CIFAR10 data from (0, 255] to (0., 1.]\n",
    "      def scale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image /= 255.0\n",
    "    return image, label\n",
    "\n",
    "  datasets, info = tfds.load(name='cifar10',\n",
    "                            with_info=True,\n",
    "                            as_supervised=True)\n",
    "   return datasets['train'].map(scale).cache().shuffle(BUFFER_SIZE).repeat()\n",
    "\n",
    "\n",
    "# Build the Keras model\n",
    "def build_and_compile_cnn_model():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(32, 32, 3)),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "      tf.keras.layers.MaxPooling2D(),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(10, activation='softmax')\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
    "      optimizer=tf.keras.optimizers.SGD(learning_rate=args.lr),\n",
    "      metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "# Train the model\n",
    "NUM_WORKERS = strategy.num_replicas_in_sync\n",
    "# Here the batch size scales up by number of workers since\n",
    "# `tf.data.Dataset.batch` expects the global batch size.\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS\n",
    "MODEL_DIR = os.getenv(\"AIP_MODEL_DIR\")\n",
    "\n",
    "train_dataset = make_datasets_unbatched().batch(GLOBAL_BATCH_SIZE)\n",
    "\n",
    "with strategy.scope():\n",
    "  # Creation of dataset, and model building/compiling need to be within\n",
    "  # `strategy.scope()`.\n",
    "  model = build_and_compile_cnn_model()\n",
    "\n",
    "model.fit(x=train_dataset, epochs=args.epochs, steps_per_epoch=args.steps)\n",
    "model.save(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job_args"
   },
   "source": [
    "### Arguments d'entraînement\n",
    "\n",
    "Le code que l'on a écrit prend les arguments suivant en entrée :\n",
    "\n",
    "- `args`: \n",
    "  - `\"--epochs=\" + EPOCHS`: Nombre d'epochs.\n",
    "  - `\"--steps=\" + STEPS`: Nombre de lot par epoch.\n",
    "  - `\"--distribute=\" + TRAIN_STRATEGY\"` : Stratégie de distributions des calculs pour l'entraînement.\n",
    "     - `\"single\"`: une seule machine.\n",
    "     - `\"mirror\"`: tous les GPU sur une seule instance de calcul.\n",
    "     - `\"multi\"`: tous les GPU sur toutes les instances de calcul.\n",
    "     \n",
    "On précisera dans la cellule suivante les paramètres à utiliser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1npiDcUtlugw"
   },
   "outputs": [],
   "source": [
    "JOB_NAME = \"custom_job_unique\"\n",
    "MODEL_DIR = \"{}/{}\".format(BUCKET_URI, JOB_NAME)\n",
    "\n",
    "\n",
    "TRAIN_STRATEGY = \"single\"\n",
    "\n",
    "EPOCHS = 60\n",
    "STEPS = 100\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--steps=\" + str(STEPS),\n",
    "    \"--distribute=\" + TRAIN_STRATEGY,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "train_custom_job"
   },
   "source": [
    "### Entraîner le modèle\n",
    "\n",
    "Nous allons maintenant définir un job d'entraînement.\n",
    "\n",
    "On utilise la classe `CustomTrainingJob` qui prend les paramètres suivant :\n",
    "\n",
    "- `display_name`: Le nom du job à définir.\n",
    "- `script_path`: Le chemin vers le script `task.py` à utiliser.\n",
    "- `container_uri`: The URI of the training container image.\n",
    "- `requirements`: The list of Python package dependencies of the script.\n",
    "- `model_serving_container_image_uri`: L'adresse de l'image du conteneur à utiliser, pré-construit ou personnalisé.\n",
    "\n",
    "On peut ensuite lancer le job avec la fonction `run`, il faut préciser les paramètres suivant :\n",
    "\n",
    "- `args`: Les arguments du script python que l'on a renseigné.\n",
    "- `replica_count`: Le nombre de worker.\n",
    "- `model_display_name`: Le nom à afficher du `Model` que le script va produire.\n",
    "- `machine_type`: Le type de machine. ex: \"n1-standard-8\"\n",
    "- `accelerator_type`: Le type d'accelerateur (GPU, TPU). ex: \"NVIDIA_TESLA_T4\"\n",
    "- `accelerator_count`: Le nombre d'accélérateur à attacher par worker. ex: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "mxIxvDdglugx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training script copied to:\n",
      "gs://cours3bucket/aiplatform-2025-02-24-15:46:18.346-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "Training Output directory:\n",
      "gs://cours3bucket/aiplatform-custom-training-2025-02-24-15:46:19.215 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5020691115460788224?project=837843744498\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5241140997806620672?project=837843744498\n",
      "CustomTrainingJob projects/837843744498/locations/us-central1/trainingPipelines/5020691115460788224 current state:\n",
      "3\n",
      "CustomTrainingJob projects/837843744498/locations/us-central1/trainingPipelines/5020691115460788224 current state:\n",
      "3\n",
      "CustomTrainingJob projects/837843744498/locations/us-central1/trainingPipelines/5020691115460788224 current state:\n",
      "3\n",
      "CustomTrainingJob projects/837843744498/locations/us-central1/trainingPipelines/5020691115460788224 current state:\n",
      "3\n",
      "CustomTrainingJob projects/837843744498/locations/us-central1/trainingPipelines/5020691115460788224 current state:\n",
      "3\n",
      "CustomTrainingJob projects/837843744498/locations/us-central1/trainingPipelines/5020691115460788224 current state:\n",
      "3\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Training failed with:\ncode: 3\nmessage: \"The replica workerpool0-0 exited with a non-zero status of 1. Termination reason: Error. \\nTraceback (most recent call last):\\n  File \\\"/opt/conda/lib/python3.7/runpy.py\\\", line 183, in _run_module_as_main\\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\\n  File \\\"/opt/conda/lib/python3.7/runpy.py\\\", line 153, in _get_module_details\\n    code = loader.get_code(mod_name)\\n  File \\\"<frozen importlib._bootstrap_external>\\\", line 860, in get_code\\n  File \\\"<frozen importlib._bootstrap_external>\\\", line 791, in source_to_code\\n  File \\\"<frozen importlib._bootstrap>\\\", line 219, in _call_with_frames_removed\\n  File \\\"/root/.local/lib/python3.7/site-packages/aiplatform_custom_trainer_script/task.py\\\", line 53\\n    image = tf.cast(image, tf.float32)\\n                                     ^\\nIndentationError: unindent does not match any outer indentation level\\n\\nTo find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=837843744498&resource=ml_job%2Fjob_id%2F5241140997806620672&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%225241140997806620672%22\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m MODEL_DISPLAY_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_unique\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Start the training\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m model \u001b[38;5;241m=\u001b[39m job\u001b[38;5;241m.\u001b[39mrun(\n\u001b[0;32m     14\u001b[0m     model_display_name\u001b[38;5;241m=\u001b[39mMODEL_DISPLAY_NAME,\n\u001b[0;32m     15\u001b[0m     args\u001b[38;5;241m=\u001b[39mCMDARGS,\n\u001b[0;32m     16\u001b[0m     replica_count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m     17\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\google\\cloud\\aiplatform\\training_jobs.py:3466\u001b[0m, in \u001b[0;36mCustomTrainingJob.run\u001b[1;34m(self, dataset, annotation_schema_uri, model_display_name, model_labels, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, base_output_dir, service_account, network, bigquery_destination, args, environment_variables, replica_count, machine_type, accelerator_type, accelerator_count, boot_disk_type, boot_disk_size_gb, reduction_server_replica_count, reduction_server_machine_type, reduction_server_container_uri, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, timeout, restart_job_on_worker_restart, enable_web_access, enable_dashboard_access, tensorboard, sync, create_request_timeout, disable_retries, persistent_resource_id, tpu_topology, scheduling_strategy, reservation_affinity_type, reservation_affinity_key, reservation_affinity_values, max_wait_duration)\u001b[0m\n\u001b[0;32m   3461\u001b[0m \u001b[38;5;66;03m# make and copy package\u001b[39;00m\n\u001b[0;32m   3462\u001b[0m python_packager \u001b[38;5;241m=\u001b[39m source_utils\u001b[38;5;241m.\u001b[39m_TrainingScriptPythonPackager(\n\u001b[0;32m   3463\u001b[0m     script_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_script_path, requirements\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requirements\n\u001b[0;32m   3464\u001b[0m )\n\u001b[1;32m-> 3466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run(\n\u001b[0;32m   3467\u001b[0m     python_packager\u001b[38;5;241m=\u001b[39mpython_packager,\n\u001b[0;32m   3468\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m   3469\u001b[0m     annotation_schema_uri\u001b[38;5;241m=\u001b[39mannotation_schema_uri,\n\u001b[0;32m   3470\u001b[0m     worker_pool_specs\u001b[38;5;241m=\u001b[39mworker_pool_specs,\n\u001b[0;32m   3471\u001b[0m     managed_model\u001b[38;5;241m=\u001b[39mmanaged_model,\n\u001b[0;32m   3472\u001b[0m     model_id\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[0;32m   3473\u001b[0m     parent_model\u001b[38;5;241m=\u001b[39mparent_model,\n\u001b[0;32m   3474\u001b[0m     is_default_version\u001b[38;5;241m=\u001b[39mis_default_version,\n\u001b[0;32m   3475\u001b[0m     model_version_aliases\u001b[38;5;241m=\u001b[39mmodel_version_aliases,\n\u001b[0;32m   3476\u001b[0m     model_version_description\u001b[38;5;241m=\u001b[39mmodel_version_description,\n\u001b[0;32m   3477\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   3478\u001b[0m     environment_variables\u001b[38;5;241m=\u001b[39menvironment_variables,\n\u001b[0;32m   3479\u001b[0m     base_output_dir\u001b[38;5;241m=\u001b[39mbase_output_dir,\n\u001b[0;32m   3480\u001b[0m     service_account\u001b[38;5;241m=\u001b[39mservice_account,\n\u001b[0;32m   3481\u001b[0m     network\u001b[38;5;241m=\u001b[39mnetwork,\n\u001b[0;32m   3482\u001b[0m     bigquery_destination\u001b[38;5;241m=\u001b[39mbigquery_destination,\n\u001b[0;32m   3483\u001b[0m     training_fraction_split\u001b[38;5;241m=\u001b[39mtraining_fraction_split,\n\u001b[0;32m   3484\u001b[0m     validation_fraction_split\u001b[38;5;241m=\u001b[39mvalidation_fraction_split,\n\u001b[0;32m   3485\u001b[0m     test_fraction_split\u001b[38;5;241m=\u001b[39mtest_fraction_split,\n\u001b[0;32m   3486\u001b[0m     training_filter_split\u001b[38;5;241m=\u001b[39mtraining_filter_split,\n\u001b[0;32m   3487\u001b[0m     validation_filter_split\u001b[38;5;241m=\u001b[39mvalidation_filter_split,\n\u001b[0;32m   3488\u001b[0m     test_filter_split\u001b[38;5;241m=\u001b[39mtest_filter_split,\n\u001b[0;32m   3489\u001b[0m     predefined_split_column_name\u001b[38;5;241m=\u001b[39mpredefined_split_column_name,\n\u001b[0;32m   3490\u001b[0m     timestamp_split_column_name\u001b[38;5;241m=\u001b[39mtimestamp_split_column_name,\n\u001b[0;32m   3491\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[0;32m   3492\u001b[0m     restart_job_on_worker_restart\u001b[38;5;241m=\u001b[39mrestart_job_on_worker_restart,\n\u001b[0;32m   3493\u001b[0m     enable_web_access\u001b[38;5;241m=\u001b[39menable_web_access,\n\u001b[0;32m   3494\u001b[0m     enable_dashboard_access\u001b[38;5;241m=\u001b[39menable_dashboard_access,\n\u001b[0;32m   3495\u001b[0m     tensorboard\u001b[38;5;241m=\u001b[39mtensorboard,\n\u001b[0;32m   3496\u001b[0m     reduction_server_container_uri\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   3497\u001b[0m         reduction_server_container_uri\n\u001b[0;32m   3498\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m reduction_server_replica_count \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3499\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3500\u001b[0m     ),\n\u001b[0;32m   3501\u001b[0m     sync\u001b[38;5;241m=\u001b[39msync,\n\u001b[0;32m   3502\u001b[0m     create_request_timeout\u001b[38;5;241m=\u001b[39mcreate_request_timeout,\n\u001b[0;32m   3503\u001b[0m     disable_retries\u001b[38;5;241m=\u001b[39mdisable_retries,\n\u001b[0;32m   3504\u001b[0m     persistent_resource_id\u001b[38;5;241m=\u001b[39mpersistent_resource_id,\n\u001b[0;32m   3505\u001b[0m     scheduling_strategy\u001b[38;5;241m=\u001b[39mscheduling_strategy,\n\u001b[0;32m   3506\u001b[0m     max_wait_duration\u001b[38;5;241m=\u001b[39mmax_wait_duration,\n\u001b[0;32m   3507\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\google\\cloud\\aiplatform\\base.py:863\u001b[0m, in \u001b[0;36moptional_sync.<locals>.optional_run_in_thread.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m    862\u001b[0m         VertexAiResourceNounWithFutureManager\u001b[38;5;241m.\u001b[39mwait(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    865\u001b[0m \u001b[38;5;66;03m# callbacks to call within the Future (in same Thread)\u001b[39;00m\n\u001b[0;32m    866\u001b[0m internal_callbacks \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\google\\cloud\\aiplatform\\training_jobs.py:4270\u001b[0m, in \u001b[0;36mCustomTrainingJob._run\u001b[1;34m(self, python_packager, dataset, annotation_schema_uri, worker_pool_specs, managed_model, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, args, environment_variables, base_output_dir, service_account, network, bigquery_destination, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, timeout, restart_job_on_worker_restart, enable_web_access, enable_dashboard_access, tensorboard, reduction_server_container_uri, sync, create_request_timeout, block, disable_retries, persistent_resource_id, scheduling_strategy, max_wait_duration)\u001b[0m\n\u001b[0;32m   4246\u001b[0m             spec[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython_package_spec\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   4247\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: value}\n\u001b[0;32m   4248\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m environment_variables\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m   4249\u001b[0m             ]\n\u001b[0;32m   4251\u001b[0m (\n\u001b[0;32m   4252\u001b[0m     training_task_inputs,\n\u001b[0;32m   4253\u001b[0m     base_output_dir,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4267\u001b[0m     max_wait_duration\u001b[38;5;241m=\u001b[39mmax_wait_duration,\n\u001b[0;32m   4268\u001b[0m )\n\u001b[1;32m-> 4270\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_job(\n\u001b[0;32m   4271\u001b[0m     training_task_definition\u001b[38;5;241m=\u001b[39mschema\u001b[38;5;241m.\u001b[39mtraining_job\u001b[38;5;241m.\u001b[39mdefinition\u001b[38;5;241m.\u001b[39mcustom_task,\n\u001b[0;32m   4272\u001b[0m     training_task_inputs\u001b[38;5;241m=\u001b[39mtraining_task_inputs,\n\u001b[0;32m   4273\u001b[0m     dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m   4274\u001b[0m     annotation_schema_uri\u001b[38;5;241m=\u001b[39mannotation_schema_uri,\n\u001b[0;32m   4275\u001b[0m     training_fraction_split\u001b[38;5;241m=\u001b[39mtraining_fraction_split,\n\u001b[0;32m   4276\u001b[0m     validation_fraction_split\u001b[38;5;241m=\u001b[39mvalidation_fraction_split,\n\u001b[0;32m   4277\u001b[0m     test_fraction_split\u001b[38;5;241m=\u001b[39mtest_fraction_split,\n\u001b[0;32m   4278\u001b[0m     training_filter_split\u001b[38;5;241m=\u001b[39mtraining_filter_split,\n\u001b[0;32m   4279\u001b[0m     validation_filter_split\u001b[38;5;241m=\u001b[39mvalidation_filter_split,\n\u001b[0;32m   4280\u001b[0m     test_filter_split\u001b[38;5;241m=\u001b[39mtest_filter_split,\n\u001b[0;32m   4281\u001b[0m     predefined_split_column_name\u001b[38;5;241m=\u001b[39mpredefined_split_column_name,\n\u001b[0;32m   4282\u001b[0m     timestamp_split_column_name\u001b[38;5;241m=\u001b[39mtimestamp_split_column_name,\n\u001b[0;32m   4283\u001b[0m     model\u001b[38;5;241m=\u001b[39mmanaged_model,\n\u001b[0;32m   4284\u001b[0m     model_id\u001b[38;5;241m=\u001b[39mmodel_id,\n\u001b[0;32m   4285\u001b[0m     parent_model\u001b[38;5;241m=\u001b[39mparent_model,\n\u001b[0;32m   4286\u001b[0m     is_default_version\u001b[38;5;241m=\u001b[39mis_default_version,\n\u001b[0;32m   4287\u001b[0m     model_version_aliases\u001b[38;5;241m=\u001b[39mmodel_version_aliases,\n\u001b[0;32m   4288\u001b[0m     model_version_description\u001b[38;5;241m=\u001b[39mmodel_version_description,\n\u001b[0;32m   4289\u001b[0m     gcs_destination_uri_prefix\u001b[38;5;241m=\u001b[39mbase_output_dir,\n\u001b[0;32m   4290\u001b[0m     bigquery_destination\u001b[38;5;241m=\u001b[39mbigquery_destination,\n\u001b[0;32m   4291\u001b[0m     create_request_timeout\u001b[38;5;241m=\u001b[39mcreate_request_timeout,\n\u001b[0;32m   4292\u001b[0m     block\u001b[38;5;241m=\u001b[39mblock,\n\u001b[0;32m   4293\u001b[0m )\n\u001b[0;32m   4295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\google\\cloud\\aiplatform\\training_jobs.py:854\u001b[0m, in \u001b[0;36m_TrainingJob._run_job\u001b[1;34m(self, training_task_definition, training_task_inputs, dataset, training_fraction_split, validation_fraction_split, test_fraction_split, training_filter_split, validation_filter_split, test_filter_split, predefined_split_column_name, timestamp_split_column_name, annotation_schema_uri, model, model_id, parent_model, is_default_version, model_version_aliases, model_version_description, gcs_destination_uri_prefix, bigquery_destination, create_request_timeout, block)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource \u001b[38;5;241m=\u001b[39m training_pipeline\n\u001b[0;32m    852\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mView Training:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dashboard_uri())\n\u001b[1;32m--> 854\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_model(block\u001b[38;5;241m=\u001b[39mblock)\n\u001b[0;32m    856\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    857\u001b[0m     _LOGGER\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[0;32m    858\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining did not produce a Managed Model returning None. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    859\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_model_upload_fail_string\n\u001b[0;32m    860\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\google\\cloud\\aiplatform\\training_jobs.py:941\u001b[0m, in \u001b[0;36m_TrainingJob._get_model\u001b[1;34m(self, block)\u001b[0m\n\u001b[0;32m    931\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to get and instantiate the Model to Upload.\u001b[39;00m\n\u001b[0;32m    932\u001b[0m \n\u001b[0;32m    933\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    938\u001b[0m \u001b[38;5;124;03m    RuntimeError: If Training failed.\u001b[39;00m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m--> 941\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_block_until_complete()\n\u001b[0;32m    943\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_failed:\n\u001b[0;32m    944\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    945\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Pipeline \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresource_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m failed. No model available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    946\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\google\\cloud\\aiplatform\\training_jobs.py:984\u001b[0m, in \u001b[0;36m_TrainingJob._block_until_complete\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    981\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wait_callback()\n\u001b[0;32m    982\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(_JOB_WAIT_TIME)\n\u001b[1;32m--> 984\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_raise_failure()\n\u001b[0;32m    986\u001b[0m _LOGGER\u001b[38;5;241m.\u001b[39mlog_action_completed_against_resource(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompleted\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39mmodel_to_upload \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_failed:\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\google\\cloud\\aiplatform\\training_jobs.py:1001\u001b[0m, in \u001b[0;36m_TrainingJob._raise_failure\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Helper method to raise failure if TrainingPipeline fails.\u001b[39;00m\n\u001b[0;32m    995\u001b[0m \n\u001b[0;32m    996\u001b[0m \u001b[38;5;124;03mRaises:\u001b[39;00m\n\u001b[0;32m    997\u001b[0m \u001b[38;5;124;03m    RuntimeError: If training failed.\u001b[39;00m\n\u001b[0;32m    998\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39merror\u001b[38;5;241m.\u001b[39mcode \u001b[38;5;241m!=\u001b[39m code_pb2\u001b[38;5;241m.\u001b[39mOK:\n\u001b[1;32m-> 1001\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining failed with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gca_resource\u001b[38;5;241m.\u001b[39merror)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Training failed with:\ncode: 3\nmessage: \"The replica workerpool0-0 exited with a non-zero status of 1. Termination reason: Error. \\nTraceback (most recent call last):\\n  File \\\"/opt/conda/lib/python3.7/runpy.py\\\", line 183, in _run_module_as_main\\n    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\\n  File \\\"/opt/conda/lib/python3.7/runpy.py\\\", line 153, in _get_module_details\\n    code = loader.get_code(mod_name)\\n  File \\\"<frozen importlib._bootstrap_external>\\\", line 860, in get_code\\n  File \\\"<frozen importlib._bootstrap_external>\\\", line 791, in source_to_code\\n  File \\\"<frozen importlib._bootstrap>\\\", line 219, in _call_with_frames_removed\\n  File \\\"/root/.local/lib/python3.7/site-packages/aiplatform_custom_trainer_script/task.py\\\", line 53\\n    image = tf.cast(image, tf.float32)\\n                                     ^\\nIndentationError: unindent does not match any outer indentation level\\n\\nTo find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=837843744498&resource=ml_job%2Fjob_id%2F5241140997806620672&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%225241140997806620672%22\"\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    script_path=\"task.py\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    requirements=[\"tensorflow_datasets==1.3.0\"],\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    ")\n",
    "\n",
    "MODEL_DISPLAY_NAME = \"model_unique\"\n",
    "\n",
    "# Start the training\n",
    "\n",
    "model = job.run(\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tarfile (from versions: none)\n",
      "ERROR: No matching distribution found for tarfile\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "make_prediction"
   },
   "source": [
    "## Prédiction par lot\n",
    "\n",
    "Nous allond maintenant utiliser le modèle pour faire de la prédiction par lot (batch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "get_test_item:test"
   },
   "source": [
    "### Données de test\n",
    "\n",
    "#### téléchargement des données\n",
    "\n",
    "Télécharger et préparer les images du CIFAR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "E1EQBPGnlugz"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_2_8.jpg...\n",
      "/ [0/10 files][    0.0 B/  8.7 KiB]   0% Done                                   \n",
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_2_5.jpg...\n",
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_0_6.jpg...\n",
      "/ [0/10 files][    0.0 B/  8.7 KiB]   0% Done                                   \n",
      "/ [0/10 files][    0.0 B/  8.7 KiB]   0% Done                                   \n",
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_2_1.jpg...\n",
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_4_10.jpg...\n",
      "/ [0/10 files][    0.0 B/  8.7 KiB]   0% Done                                   \n",
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_2_9.jpg...\n",
      "/ [0/10 files][    0.0 B/  8.7 KiB]   0% Done                                   \n",
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_5_4.jpg...\n",
      "/ [0/10 files][    0.0 B/  8.7 KiB]   0% Done                                   \n",
      "/ [0/10 files][    0.0 B/  8.7 KiB]   0% Done                                   \n",
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_0_3.jpg...\n",
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_7_2.jpg...\n",
      "/ [0/10 files][    0.0 B/  8.7 KiB]   0% Done                                   \n",
      "Copying gs://cloud-samples-data/ai-platform-unified/cifar_test_images/image_3_7.jpg...\n",
      "/ [0/10 files][    0.0 B/  8.7 KiB]   0% Done                                   \n",
      "/ [0/10 files][    0.0 B/  8.7 KiB]   0% Done                                   \n",
      "/ [0/10 files][  951.0 B/  8.7 KiB]  10% Done                                   \n",
      "-\n",
      "- [1/10 files][  951.0 B/  8.7 KiB]  10% Done                                   \n",
      "- [2/10 files][  1.8 KiB/  8.7 KiB]  20% Done                                   \n",
      "- [3/10 files][  2.7 KiB/  8.7 KiB]  30% Done                                   \n",
      "- [4/10 files][  3.6 KiB/  8.7 KiB]  40% Done                                   \n",
      "- [5/10 files][  4.5 KiB/  8.7 KiB]  51% Done                                   \n",
      "- [6/10 files][  5.4 KiB/  8.7 KiB]  61% Done                                   \n",
      "- [7/10 files][  6.2 KiB/  8.7 KiB]  71% Done                                   \n",
      "- [8/10 files][  7.9 KiB/  8.7 KiB]  90% Done                                   \n",
      "- [9/10 files][  7.9 KiB/  8.7 KiB]  90% Done                                   \n",
      "- [10/10 files][  8.7 KiB/  8.7 KiB] 100% Done                                  \n",
      "\n",
      "Operation completed over 10 objects/8.7 KiB.                                     \n"
     ]
    }
   ],
   "source": [
    "# Download the images\n",
    "! gsutil -m cp -r gs://cloud-samples-data/ai-platform-unified/cifar_test_images ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "cl59KGnXlugz"
   },
   "outputs": [],
   "source": [
    "# Load image data\n",
    "IMAGE_DIRECTORY = \"cifar_test_images\"\n",
    "\n",
    "image_files = [file for file in os.listdir(IMAGE_DIRECTORY) if file.endswith(\".jpg\")]\n",
    "\n",
    "# Decode JPEG images into numpy arrays\n",
    "image_data = [\n",
    "    np.asarray(Image.open(os.path.join(IMAGE_DIRECTORY, file))) for file in image_files\n",
    "]\n",
    "\n",
    "# Scale and convert to expected format\n",
    "x_test = [(image / 255.0).astype(np.float32).tolist() for image in image_data]\n",
    "\n",
    "# Extract labels from image name\n",
    "y_test = [int(file.split(\"_\")[1]) for file in image_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b1e29665076f"
   },
   "source": [
    "#### Préparer les données à la prediction par lot\n",
    "Il existe plusieurs formats possibles pour l'enregistrement des données d'entrée d'un job de batch prediction.\n",
    "\n",
    "On utilise ici le format JSONL.\n",
    "\n",
    "Pour plus d'information : https://cloud.google.com/vertex-ai/docs/predictions/batch-predictions#batch_request_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "3e6b04d29c3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded instances to:  gs://cours3bucket/batch_prediction_instances/batch_prediction_instances.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file://batch_prediction_instances.jsonl [Content-Type=application/octet-stream]...\n",
      "/ [0 files][    0.0 B/617.6 KiB]                                                \n",
      "/ [1 files][617.6 KiB/617.6 KiB]                                                \n",
      "-\n",
      "\n",
      "Operation completed over 1 objects/617.6 KiB.                                    \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "BATCH_PREDICTION_INSTANCES_FILE = \"batch_prediction_instances.jsonl\"\n",
    "\n",
    "BATCH_PREDICTION_GCS_SOURCE = (\n",
    "    BUCKET_URI + \"/batch_prediction_instances/\" + BATCH_PREDICTION_INSTANCES_FILE\n",
    ")\n",
    "\n",
    "# Write instances at JSONL\n",
    "with open(BATCH_PREDICTION_INSTANCES_FILE, \"w\") as f:\n",
    "    for x in x_test:\n",
    "        f.write(json.dumps(x) + \"\\n\")\n",
    "\n",
    "# Upload to Cloud Storage bucket\n",
    "! gsutil cp $BATCH_PREDICTION_INSTANCES_FILE $BATCH_PREDICTION_GCS_SOURCE\n",
    "\n",
    "print(\"Uploaded instances to: \", BATCH_PREDICTION_GCS_SOURCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "send_prediction_request:image"
   },
   "source": [
    "### Envoi de la requète de prédiction\n",
    "\n",
    "Pour faire une requète de prédiction par lot, on appelle la méthode batch_predict du modèle avec les paramètres suivants :\n",
    "\n",
    "- `instances_format` : Le format du fichier de demande de prédiction par lot : \"jsonl\", \"csv\", \"bigquery\", \"tf-record\", \"tf-record-gzip\" ou \"file-list\"\n",
    "- `prediction_format` : Le format du fichier de réponse de prédiction par lot : \"jsonl\", \"csv\", \"bigquery\", \"tf-record\", \"tf-record-gzip\" ou \"file-list\"\n",
    "- `job_display_name` : Le nom de la tâche de prédiction.\n",
    "- `gcs_source` : Une liste d'un ou plusieurs chemins Cloud Storage vers les données sources. \n",
    "- `gcs_destination_prefix` : Le chemin Cloud Storage où le service écrira les prédictions.\n",
    "- `model_parameters` : Paramètres de filtrage supplémentaires pour la diffusion des résultats de prédiction.\n",
    "- `machine_type` : Le type de machine à utiliser pour l'entraînement.\n",
    "- `accelerator_type` : Le type d'accélérateur matériel.\n",
    "- `accelerator_count` : Le nombre d'accélérateurs à attacher à une réplique de travail.\n",
    "- `starting_replica_count` : Le nombre d'instances de calcul à provisionner initialement.\n",
    "- `max_replica_count` : Le nombre maximum d'instances de calcul auxquelles effectuer la mise à l'échelle. Dans ce tutoriel, une seule instance est provisionnée.\n",
    "\n",
    "### Mise à l'échelle des instances de calcul\n",
    "On utilise pour l'instant une seule instance (ou nœud) pour traiter les requêtes de prédiction par lot : les variables `MIN_NODES` et `MAX_NODES` sont toutes deux fixées à `1`.\n",
    "\n",
    "Pour utiliser plusieurs nœuds dans la prédiction par lot, on peut définir `MAX_NODES` sur le nombre maximum de nœuds que l'on souhaite utiliser. Vertex AI met automatiquement à l'échelle le nombre de nœuds utilisés pour servir les prédictions, jusqu'au nombre maximum défini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1cf1076178fc"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m BATCH_PREDICTION_GCS_DEST_PREFIX \u001b[38;5;241m=\u001b[39m BUCKET_URI \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m DESTINATION_FOLDER\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Make SDK batch_predict method call\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m batch_prediction_job \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbatch_predict(\n\u001b[0;32m     15\u001b[0m     instances_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     predictions_format\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     17\u001b[0m     job_display_name\u001b[38;5;241m=\u001b[39mBATCH_PREDICTION_JOB_NAME,\n\u001b[0;32m     18\u001b[0m     gcs_source\u001b[38;5;241m=\u001b[39mBATCH_PREDICTION_GCS_SOURCE,\n\u001b[0;32m     19\u001b[0m     gcs_destination_prefix\u001b[38;5;241m=\u001b[39mBATCH_PREDICTION_GCS_DEST_PREFIX,\n\u001b[0;32m     20\u001b[0m     model_parameters\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     21\u001b[0m     starting_replica_count\u001b[38;5;241m=\u001b[39mMIN_NODES,\n\u001b[0;32m     22\u001b[0m     max_replica_count\u001b[38;5;241m=\u001b[39mMAX_NODES,\n\u001b[0;32m     23\u001b[0m     machine_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn1-standard-4\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m     sync\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     25\u001b[0m     spot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     26\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "# The name of the job\n",
    "BATCH_PREDICTION_JOB_NAME = \"cifar10_batch_prediction_unique\"\n",
    "\n",
    "# Folder in the bucket to write results to\n",
    "DESTINATION_FOLDER = \"batch_prediction_results\"\n",
    "\n",
    "# The Cloud Storage bucket to upload results to\n",
    "BATCH_PREDICTION_GCS_DEST_PREFIX = BUCKET_URI + \"/\" + DESTINATION_FOLDER\n",
    "\n",
    "# Make SDK batch_predict method call\n",
    "batch_prediction_job = model.batch_predict(\n",
    "    instances_format=\"jsonl\",\n",
    "    predictions_format=\"jsonl\",\n",
    "    job_display_name=BATCH_PREDICTION_JOB_NAME,\n",
    "    gcs_source=BATCH_PREDICTION_GCS_SOURCE,\n",
    "    gcs_destination_prefix=BATCH_PREDICTION_GCS_DEST_PREFIX,\n",
    "    model_parameters=None,\n",
    "    starting_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    sync=True,\n",
    "    spot=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "df3b3d1bdd24"
   },
   "source": [
    "### Récupérer les résultats\n",
    "\n",
    "A la fin du job de prédiction, les résultats sont accessibles dans le répertoire de sortie que l'on a précisé dans un jsonl appelé prediction.results-xxxx-of-xxxx.\n",
    "\n",
    "Pour chaque image, on obtient un vecteur de probabilité pour chacune des 10 classes du dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2f10b13b2b88"
   },
   "outputs": [],
   "source": [
    "RESULTS_DIRECTORY = \"prediction_results\"\n",
    "RESULTS_DIRECTORY_FULL = RESULTS_DIRECTORY + \"/\" + DESTINATION_FOLDER\n",
    "\n",
    "# Create missing directories\n",
    "os.makedirs(RESULTS_DIRECTORY, exist_ok=True)\n",
    "\n",
    "# Get the Cloud Storage paths for each result\n",
    "! gsutil -m cp -r $BATCH_PREDICTION_GCS_DEST_PREFIX $RESULTS_DIRECTORY\n",
    "\n",
    "# Get most recently modified directory\n",
    "latest_directory = max(\n",
    "    (\n",
    "        os.path.join(RESULTS_DIRECTORY_FULL, d)\n",
    "        for d in os.listdir(RESULTS_DIRECTORY_FULL)\n",
    "    ),\n",
    "    key=os.path.getmtime,\n",
    ")\n",
    "\n",
    "# Get downloaded results in directory\n",
    "results_files = []\n",
    "for dirpath, subdirs, files in os.walk(latest_directory):\n",
    "    for file in files:\n",
    "        if file.startswith(\"prediction.results\"):\n",
    "            results_files.append(os.path.join(dirpath, file))\n",
    "\n",
    "# Consolidate all the results into a list\n",
    "results = []\n",
    "for results_file in results_files:\n",
    "    # Download each result\n",
    "    with open(results_file, \"r\") as file:\n",
    "        results.extend([json.loads(line) for line in file.readlines()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "962b5a11fdae"
   },
   "source": [
    "### Evaluer les résulats\n",
    "\n",
    "Pour l'évaluation :\n",
    "1. `np.argmax`: sélectionne le label prédit (avec la plus grande probabilité)\n",
    "2. On compare ensuite la prédiction au label d'origine\n",
    "3. On calcule l'`accuracy` $=$ `correct/total`\n",
    "\n",
    "On pourra essayer d'autres paramètres d'entrainement pour augmenter les performances du modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "UywuX7fRlugz"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (10,) (0,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m y_predicted \u001b[38;5;241m=\u001b[39m [np\u001b[38;5;241m.\u001b[39margmax(result[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprediction\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results]\n\u001b[1;32m----> 3\u001b[0m correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(y_predicted \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39marray(y_test))\n\u001b[0;32m      4\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(y_predicted)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCorrect predictions = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Total predictions = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect\u001b[38;5;241m/\u001b[39maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (10,) (0,) "
     ]
    }
   ],
   "source": [
    "y_predicted = [np.argmax(result[\"prediction\"]) for result in results]\n",
    "\n",
    "correct = sum(y_predicted == np.array(y_test))\n",
    "accuracy = len(y_predicted)\n",
    "print(\n",
    "    f\"Correct predictions = {correct}, Total predictions = {accuracy}, Accuracy = {correct/accuracy}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prédiction en ligne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Déployer le modèle sur un endpoint\n",
    "\n",
    "Avant de pouvoir obtenir des prédictions, le modèle doit être déployé sur un `Endpoint`. L'endpoint est un serveur qui tourne sur les ressources qu'on définit au préalable. Il gère les requêtes qu'il reçoit au format HTTP et distribue les calculs pour une réponse avec un minimum de latence.\n",
    "\n",
    "On utilisera la fonction `deploy` qui prend en entrée :\n",
    "\n",
    "- `deployed_model_display_name`: Un nom pour le modèle deployé\n",
    "- `traffic_split`: Le pourcentage du trafic de l'endpoint qui va être transmis aux différents modèles de l'edpoint.\n",
    "   - On précisera la répartition du trafic sous forme de dictionnaire python où les clés sont les id des modèles et les valeurs sont les pourcentages de trafic qui doivent sommer à 100. On utilise \"0\" pour la clé du modèle que l'on déploie. Ex : *{ \"0\": percent, model_id: percent, ... }**\n",
    "- `machine_type`: Le type de machine pour l'entraînement.\n",
    "- `accelerator_type`: Le type d'accelerateur (GPU, TPU).\n",
    "- `accelerator_count`: Le nombre d'accelerateur par worker.\n",
    "- `starting_replica_count`: Le nombre initial d'instance de calcul.\n",
    "- `min_replica_count`: Le nombre minimum d'instances de calcul que Vertex AI doit attribuer à l'endpoint.\n",
    "- `max_replica_count`: Le nombre maximum d'instances de calcul que Vertex AI puisse attribuer à l'endpoint pour l'_autoscaling_.\n",
    "\n",
    "### Traffic split\n",
    "\n",
    "Le `traffic_split` est généralement utilisé pour introduire graduellement un nouveau modèle en production. C'est ce qu'on appelle l'A/B testing, et cela permet de comparer les performances d'un nouveau modèle en production tout en minimizant le risque d'un changement brutal pour la plupart des utilisateurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Endpoint\n",
      "Create Endpoint backing LRO: projects/837843744498/locations/us-central1/endpoints/3914675910766231552/operations/6216935508505788416\n",
      "Endpoint created. Resource name: projects/837843744498/locations/us-central1/endpoints/3914675910766231552\n",
      "To use this Endpoint in another session:\n",
      "endpoint = aiplatform.Endpoint('projects/837843744498/locations/us-central1/endpoints/3914675910766231552')\n",
      "Deploying model to Endpoint : projects/837843744498/locations/us-central1/endpoints/3914675910766231552\n",
      "Using default machine_type: n1-standard-2\n",
      "Deploy Endpoint model backing LRO: projects/837843744498/locations/us-central1/endpoints/3914675910766231552/operations/1765971701780185088\n",
      "Endpoint model deployed. Resource name: projects/837843744498/locations/us-central1/endpoints/3914675910766231552\n"
     ]
    }
   ],
   "source": [
    "DEPLOYED_NAME = \"cifar10_deployed_unique\"\n",
    "\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=DEPLOYED_NAME,\n",
    "    traffic_split=TRAFFIC_SPLIT,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Préparer les images\n",
    "\n",
    "Les images qui seront envoyés à l'endpoint doivent être _préprocessées_ afin qu'elles soient au même format que le format d'entrée du script d'entraînement `task.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Load image data\n",
    "IMAGE_DIRECTORY = \"cifar_test_images\"\n",
    "\n",
    "image_files = [file for file in os.listdir(IMAGE_DIRECTORY) if file.endswith(\".jpg\")]\n",
    "\n",
    "# Decode JPEG images into numpy arrays\n",
    "image_data = [\n",
    "    np.asarray(Image.open(os.path.join(IMAGE_DIRECTORY, file))) for file in image_files\n",
    "]\n",
    "\n",
    "# Scale and convert to expected format\n",
    "x_test = [(image / 255.0).astype(np.float32).tolist() for image in image_data]\n",
    "\n",
    "# Extract labels from image name\n",
    "y_test = [int(file.split(\"_\")[1]) for file in image_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Envoyer les requêtes de prédiction\n",
    "\n",
    "On utilise la méthode `predict` de l'objet `Endpoint`. La fonction retourne une liste où chaque élément correspond à une image de la requête. \n",
    "\n",
    "Dans la cellule suivante, on lance une évaluation des résultats de prédiction comme pour la prédiction par bacth, on ne devrait pas observer de changement de performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct predictions = 2, Total predictions = 10, Accuracy = 0.2\n"
     ]
    }
   ],
   "source": [
    "# Obtenir l'endpoint.\n",
    "# A priori, l'endpoint est déja chargé à ce stade.\n",
    "# endpoint = aiplatform.Endpoint(ENDPOINT_ID)\n",
    "\n",
    "predictions = endpoint.predict(instances=x_test)\n",
    "y_predicted = np.argmax(predictions.predictions, axis=1)\n",
    "\n",
    "correct = sum(y_predicted == np.array(y_test))\n",
    "total_predictions = len(y_predicted)\n",
    "print(\n",
    "    f\"Correct predictions = {correct}, Total predictions = {total_predictions}, Accuracy = {correct/total_predictions}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cleanup:custom"
   },
   "source": [
    "# Nettoyage du projet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NNmebHf7lug0"
   },
   "outputs": [],
   "source": [
    "# Delete the training job\n",
    "job.delete()\n",
    "\n",
    "# Delete the model\n",
    "model.delete()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sdk-custom-image-classification-batch.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
