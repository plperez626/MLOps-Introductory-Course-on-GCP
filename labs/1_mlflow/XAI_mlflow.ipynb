{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLOPs on GCP course - MLFlow introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://www.headmind.com/wp-content/uploads/2024/01/logo_dark.png width=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=https://www.isae-supaero.fr/local/cache-vignettes/L190xH102/siteon0-e5814.png width=\"200\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "*Context*\n",
    "\n",
    "Credit risk is the risk that a customer doesn't pay back the money they borrowed from a bank. Banks do credit risk modelling to minimize their expected credit loss. ML models can be trained to classify whether a customer is at risk or not.\n",
    "\n",
    "*Dataset*\n",
    "\n",
    "The German Credit Risk dataset is used.\n",
    "\n",
    "The dataset is anonymized because it contains personal identifiable information (PII) on the bank customers. The features are described in the data/features.txt file.\n",
    "\n",
    "*Objectives*\n",
    "\n",
    "- Dataset exploration : Using EDA, explore the relevant data \n",
    "- ML implementation : train a Random Forest Classifier with Optuna \n",
    "\n",
    "*Notebook made by Headmind Partners AI & Blockchain*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "import optuna\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "from IPython.display import Image\n",
    "pd.set_option(\"display.max_columns\", 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running MLFlow server\n",
    "MLFlow enables us to track several informations on the ML model runs through a UI. To start the server, use the command\n",
    "\n",
    "```mlflow server --host 127.0.0.1 --port 8080```\n",
    "\n",
    " from the root of the project\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "# By default, the logs will be saved in the current folder. To link your notebook computations to the mlflow server, set the tracking uri to the same uri as the server\n",
    "host = \"127.0.0.1\"\n",
    "port = \"5000\"\n",
    "mlflow.set_tracking_uri(uri = f\"http://{host}:{port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "filename = \"data/dataset.parquet\"\n",
    "\n",
    "df = pd.read_parquet(filename)\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The goal is to predict if a bank can give a credit to a customer according to its profile\n",
    "\n",
    "Question: Identify the target field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Identify target field\n",
    "#########################\n",
    "# target_field = \"...\"\n",
    "target_field = \"class\" # TODO\n",
    "#########################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's rename the target field\n",
    "df = df.rename(columns={target_field:'risk'})\n",
    "# And change the label values \n",
    "df['risk'] = df['risk'].map({1:0,2:1})\n",
    "\n",
    "y = df['risk']\n",
    "X = df.drop(columns=['risk'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "This is a binary classification problem where\n",
    "-  y = 1 if the customer is at risk\n",
    "-  y = 0 if the customer is \"bankable\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "In real life banks assess customer risk with more than two values (risky or not risky).\n",
    "\n",
    "In our case, what trick would you suggest to get n risk values (with n>2) ? (with probabilities for instance)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "--------------------------\n",
    "ANSWER HERE\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Using seaborn to explore data \n",
    "\n",
    "Correlation matrixes and features distributions according to the credit risk are displayed using the *seaborn* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Correlation matrix\n",
    "corr = df.corr(numeric_only = True)\n",
    "plt.figure(figsize=(12,12))\n",
    "sns.heatmap(corr, cmap=\"Blues\", annot=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Question : Do you consider the dataset unbalanced ? Compute the label proportion. If a dataset is unbalanced what are the risks on the model? Which method would you use to manage an unbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "ANSWER HERE\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Encoding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Preliminary data exploration helped us discover all the features in the dataset, their distributions and correlations.\n",
    "\n",
    "The categorical features now have to be encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numeric_feat = X.select_dtypes(include=numerics).columns.tolist()\n",
    "\n",
    "##############################################\n",
    "# Pick the right categorical features to encode\n",
    "categorical_feat = [\"checking_account_status\",\"credit_history\",\"purpose\",'savings_account',\"employment_duration\",\"personal_status_sex\",\"other_debtors\",\"property\",\"other_installment_plans\",\"housing\",\"job\",\"telephone\",\"foreign_worker\"] # TODO\n",
    "##############################################\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "\n",
    "# Fit_transform - create a X_enc dataframe from the X dataframe\n",
    "X_enc_array = onehot_encoder.fit_transform(X[categorical_feat])\n",
    "X_enc = pd.DataFrame(X_enc_array.toarray(), columns=onehot_encoder.get_feature_names_out(input_features=categorical_feat))\n",
    "X_enc[numeric_feat] = X[numeric_feat]\n",
    "\n",
    "display(X_enc.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is a one-hot encoder? How would it transform the following pandas Series: ['Cat','Cat','Dog','Cat','Bird','Dog']?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "ANSWER HERE\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/one_hot_encoder.pkl\", 'wb') as file:\n",
    "    pickle.dump(onehot_encoder, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## ML Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question : Split X and y to fit the model. Make sure the risk proportion in the train set are the same as in the test set using the argument *stratify*. Use random_state = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X_enc,y,test_size=0.2, stratify=y, random_state=16) # TODO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training an ML model\n",
    "During the rest of this workshop, we'll train a random forest classifier. What other models would be appropriate for the current problem? Justify your answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "ANSWER HERE\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with default hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic configuration\n",
    "\n",
    "rf_clf = RandomForestClassifier(random_state=42)\n",
    "rf_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mlflow.set_experiment(experiment_name=\"finetune-creditrisk\")\n",
    "with mlflow.start_run(run_name=\"RandomForest_NoOptimization\"):\n",
    "    # log params\n",
    "    params = rf_clf.get_params()\n",
    "    mlflow.log_param(\"n_estimators\", params[\"n_estimators\"])\n",
    "    mlflow.log_param(\"bootstrap\", params[\"bootstrap\"])\n",
    "    mlflow.log_param(\"min_samples_leaf\", params[\"min_samples_leaf\"])\n",
    "    mlflow.log_param(\"max_depth\", params[\"max_depth\"])\n",
    "\n",
    "    # log metrics\n",
    "    y_pred = rf_clf.predict_proba(X_test)[:,1]\n",
    "    mlflow.log_metric(\"auc\", roc_auc_score(y_test,y_pred))\n",
    "    mlflow.log_metric(\"f1-score\", f1_score(y_test, rf_clf.predict(X_test)))\n",
    "    \n",
    "    mlflow.sklearn.log_model(rf_clf, artifact_path=\"sklearn-model\",\n",
    "        registered_model_name=\"sk-learn-random-forest\")\n",
    "    \n",
    "    mlflow.log_artifact(local_path='data/one_hot_encoder.pkl', artifact_path=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using only the MLFlow UI, what are the basic parameters of a random forest classifier? (Justify by writing the path you took in the UI to read them)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "ANSWER HERE\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing hyperparameters by hand\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results of the optimization, fine-tune the model using the provided code and write each result you obtain in a table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "ANSWER HERE\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify here to fine-tune the model\n",
    "params = {\n",
    "    \"n_estimators\":100,\n",
    "    \"bootstrap\":False,\n",
    "    \"min_samples_leaf\":1,\n",
    "    \"max_depth\":4,\n",
    "}\n",
    "\n",
    "rf_clf = RandomForestClassifier(**params,random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "mlflow.set_experiment(experiment_name=\"finetune-creditrisk\")\n",
    "with mlflow.start_run(run_name=\"RandomForest_manualOptim\"):\n",
    "    # log params\n",
    "    params = rf_clf.get_params()\n",
    "    mlflow.log_param(\"n_estimators\", params[\"n_estimators\"])\n",
    "    mlflow.log_param(\"bootstrap\", params[\"bootstrap\"])\n",
    "    mlflow.log_param(\"min_samples_leaf\", params[\"min_samples_leaf\"])\n",
    "    mlflow.log_param(\"max_depth\", params[\"max_depth\"])\n",
    "\n",
    "    # log metrics\n",
    "    y_pred = rf_clf.predict_proba(X_test)[:,1]\n",
    "    mlflow.log_metric(\"auc\", roc_auc_score(y_test,y_pred))\n",
    "    mlflow.log_metric(\"f1-score\", f1_score(y_test, rf_clf.predict(X_test)))\n",
    "    \n",
    "    mlflow.sklearn.log_model(rf_clf, artifact_path=\"sklearn-model\",\n",
    "        registered_model_name=\"sk-learn-random-forest\")\n",
    "    \n",
    "    mlflow.log_artifact(local_path='data/one_hot_encoder.pkl', artifact_path=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optimizing hyperparameters with Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=https://optuna.readthedocs.io/en/stable/index.html> Optuna </a> is a hyperparameter fine-tuning framework.\n",
    "\n",
    "To use it, you first define a trial, a scoring function, and a set of hyperparameters to fine-tune, using 'suggest' methods.\n",
    "\n",
    "Then, you choose an heuristic and optuna will try different sets of hyperparameters and log the KPIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify = y_train, random_state=16)\n",
    "\n",
    "def objective_rf(trial):\n",
    "    rf_params = {\n",
    "            # Parameter space definition\n",
    "            #################################################################\n",
    "            'n_estimators' : trial.suggest_int('n_estimators',low=50,high=250),\n",
    "            'max_depth' : trial.suggest_int('max_depth',low=2,high=10),\n",
    "            'max_features' : trial.suggest_categorical('max_features', [\"sqrt\", None]),\n",
    "            'min_samples_leaf' : trial.suggest_float(\"min_samples_leaf\", low = 0.001, high = 0.1)\n",
    "            #################################################################\n",
    "            }\n",
    "\n",
    "    rf_classifier = RandomForestClassifier(random_state=42)\n",
    "    rf_classifier.set_params(**rf_params)\n",
    "\n",
    "    rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Log metrics\n",
    "    y_pred = rf_classifier.predict(X_val)\n",
    "    score=f1_score(y_val, y_pred)\n",
    "    mlflow.log_metric(\"auc\", roc_auc_score(y_val,y_pred))\n",
    "    mlflow.log_metric(\"f1-score\", score)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = optuna.create_study(direction=\"maximize\")\n",
    "full_objective = lambda trial: objective_rf(trial)\n",
    "mlflow.set_experiment(experiment_name=\"finetune-creditrisk\")\n",
    "with mlflow.start_run(run_name=\"RandomForest_Finetuning_exp\"):\n",
    "    study.optimize(full_objective, n_trials=30, timeout=600)\n",
    "rf_params = study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the difference between a train, validation, and test set. What are the risks if there is overlapping between the validation and test set?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------\n",
    "ANSWER HERE\n",
    "\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier = RandomForestClassifier(random_state=42)\n",
    "rf_classifier.set_params(**rf_params)\n",
    "\n",
    "X_train_val, y_train_val = pd.concat((X_train, X_val)), pd.concat((y_train, y_val))\n",
    "\n",
    "rf_classifier.fit(X_train_val, y_train_val)\n",
    "with mlflow.start_run(run_name=\"RandomForest_Optimization\"):\n",
    "    # log params\n",
    "    mlflow.log_param(\"n_estimators\", rf_params[\"n_estimators\"])\n",
    "    mlflow.log_param(\"min_samples_leaf\", rf_params[\"min_samples_leaf\"])\n",
    "    mlflow.log_param(\"max_depth\", rf_params[\"max_depth\"])\n",
    "    mlflow.log_param('max_features', rf_params['max_features'])\n",
    "\n",
    "    # log metrics\n",
    "    y_pred = rf_classifier.predict_proba(X_test)[:,1]\n",
    "    mlflow.log_metric(\"auc\", roc_auc_score(y_test,y_pred))\n",
    "    mlflow.log_metric(\"f1-score\", f1_score(y_test, rf_classifier.predict(X_test)))\n",
    "    \n",
    "    mlflow.sklearn.log_model(rf_clf, artifact_path=\"sklearn-model\",\n",
    "        registered_model_name=\"sk-learn-random-forest-finetuned\")\n",
    "    mlflow.log_artifact(local_path='data/one_hot_encoder.pkl', artifact_path=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieves a model logged on MLFlow - on run_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from IPython.display import display\n",
    "\n",
    "experiment_name = [\"finetune-creditrisk\"]\n",
    "run_name = \"RandomForest_Optimization\"\n",
    "\n",
    "# Search for the run using the experiment name and run name\n",
    "runs = mlflow.search_runs(experiment_names=experiment_name)\n",
    "\n",
    "display(runs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_run_id = runs.loc[runs[\"tags.mlflow.runName\"] == run_name]\n",
    "last_run_id.sort_values(by = [\"end_time\"], ascending=False, inplace=True)\n",
    "run_id = last_run_id.iloc[0][\"run_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieves a model from MLFlow\n",
    "model = mlflow.sklearn.load_model(f\"runs:/{run_id}/sklearn-model\")\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upgrades the model status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "client.transition_model_version_stage(\n",
    "    name=\"sk-learn-random-forest-finetuned\", version=1, stage=\"Production\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieves the model from the status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "model_name = \"sk-learn-random-forest-finetuned\"\n",
    "model_version = 1\n",
    "\n",
    "model = mlflow.sklearn.load_model(model_uri=f\"models:/{model_name}/{model_version}\")\n",
    "\n",
    "# Launche mlflow server at the start\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
